\graphicspath{{literature/fig/}}

\chapter{Literature Review and Theoretical Framework}
\label{chap:literature}

Advancing drought monitoring requires an understanding of both the prior research and theoretical foundation. This chapter addresses this need by first reviewing key studies that have shaped the development of composite drought indicators (Section~\ref{sec:lit_review}). It then establishes the core theoretical framework of probabilistic graphical models (PGMs) that underpins this study (Section~\ref{sec:pgm_theory}).

Together, these two perspectives---empirical and theoretical---motivate the use of PGMs as a promising tool for drought monitoring, while establishing the theoretical background for the rest of this report.

\section{Literature Review on Drought Monitoring}
\label{sec:lit_review}
Scholars have investigated methods ranging from traditional single-index approaches to more sophisticated probabilistic models. The following reviews four key studies that collectively sketch the evolution of composite drought indicator development. Two that applied dynamic naive Bayes classifiers (DNBCs) to integrate multiple drought indicators in South Korea; one that assessed the suitability of various drought indices for South Africa’s complex climatic conditions; and one that employed a dimensionality reduction technique to construct a composite drought index for New Mexico. 

The review summarises the objectives, methods, and findings of each study, followed by a synthesis that identifies the limitations of current approaches and motivates the use of PGMs for this research.

\subsection{Integrating Multi-Index Drought Data Using a DNBC}
This study by Kim et al. developed a dynamic naive Bayes classifier multiple drought index (DNBC-MDI) to generate a probabilistic and multi-dimensional assessment of drought risk. The core idea was to create one coherent model that would combine established drought indicators in order to capture the four most important dimensions of drought---that is, the standardised precipitation index (SPI) for meteorological drought, the streamflow drought index (SDI) for hydrological drought, the evapotranspiration stress index (ESI) for agricultural drought, and the water supply condition index (WSCI) for socio-economic drought. 

The model was applied to the Han River basin, using observed records from 1974--2016 and future climate projections extending to 2099 under a high-emission scenario. The DNBC was trained to recognise hidden drought states over time, adjusting its internal parameters through an iterative learning process known as expectation-maximisation (EM). This allowed the model to infer the likelihood of different drought conditions given observed changes in precipitation, streamflow, and evapotranspiration. 

To assess the relationship between different drought variables and their joint risk, the authors used a statistical function known as the \textit{Clayton copula}. This is a mathematical way of linking multiple probability distributions so that their dependencies can be captured and analysed together, rather than assuming each drought indicator behaves independently. Using this approach, they estimated long-term drought risks such as 100-year return periods, representing the probability of extreme drought events over time. 

The results showed that the DNBC-MDI achieved higher classification accuracy than any of the individual indices on their own, while successfully reproducing several major historical drought events (1994--1995, 2001, 2008--2009, 2012, 2014--2015). The authors also highlighted two main limitations: first, the model relied mainly on climate simulations and did not incorporate remote-sensing data such as satellite-derived vegetation indices; and second, it assumed that the input drought indices were conditionally independent of one another. This is an assumption that may not fully hold in reality given the physical links between rainfall, river flow, and evaporation. 

Overall, this paper demonstrated the feasibility and potential of DNBC-based models for integrating multiple drought indicators into a single probabilistic framework, while also highlighting the importance of addressing data diversity and inter-variable dependencies when adapting such methods to new contexts~\cite{dnbc_drought_second}.

\subsection{Probabilistic Frameworks for Drought Characterisation with DNBCs}
Similarly to the study above, Chen et al. also applied a DNBC to create a composite drought indicator. However, in this paper the researchers only utilised three indicators which were SPI, SDI and the normalised vegetation supply water index (NVSWI) which attempts to capture agricultural drought and is thus an alternative to the ESI. The aim was to evaluate whether this model could capture drought events more accurately and consistently than any single index on its own, particularly in terms of detection, classification, and persistence through time. It is important to note the omission of the socio-economic aspect of drought here.

The model was applied to the upper Han River Basin in South Korea using observations from 1980--2015 and satellite-based data from 2003--2015. Within this framework, the DNBC represented drought as a sequence of hidden states, each corresponding to a distinct drought severity level. These hidden states evolved over time in response to changes in the observed indices. The number of drought states was chosen using statistical model selection criteria to balance model complexity and interpretability. Parameter estimation relied on the EM algorithm, similar to the study above.

The results showed that the DNBC successfully reproduced several well-documented drought episodes (2004, 2006, 2008--2009, 2014, 2015) and accurately reflected both drought duration and persistence. In comparative testing, the DNBC-based drought states detected nearly all events identified by the individual indices, matching or exceeding their detection rates across the different drought types. The analysis also revealed that meteorological and hydrological indicators were more closely related to one another than to the agricultural indicator, illustrating the complexity of cross-sector drought linkages. Overall, the DNBC provided a coherent probabilistic framework that incorporated uncertainty directly into drought monitoring.

Nonetheless, the authors noted several limitations. The model was limited to three indices, omitting potentially informative climatic variables such as temperature, water vapour, and solar radiation, which may have strengthened its predictive capacity. Furthermore, the DNBC assumed conditional independence among its input indicators, an assumption that may oversimplify the real-world interconnections between atmospheric, hydrological, and vegetation processes. 

Despite these challenges, the study clearly demonstrates how DNBCs can unify multiple drought indicators and provides a methodological framework for developing composite indices, thus forming the foundation of this study~\cite{dnbc_drought_first}. 

\subsection{Evaluating Drought Indices for South African Conditions}
Mukhawana et al. conducted a review that evaluated existing drought indices in order to identify the most effective and feasible ones for integrated drought monitoring in South Africa’s diverse climate. It examined eight widely used indicators, each representing different aspects of drought. Their goal was to determine which indicators could realistically be applied in the South African context, given the country’s sparsity of data.

Following the World Meteorological Organisation’s (WMO) 2016 guidelines, the review assessed each index against five criteria: data requirements, computational simplicity, sensitivity to drought conditions, adaptability for integration, and overall reliability. The evaluation was based on existing studies conducted across South Africa and other regions with comparable climates.

The findings revealed that indices relying on detailed soil and surface-water data (such as the Palmer drought severity index and surface water supply index) are not feasible due to the lack of consistent data. In contrast, indicators derived from rainfall, and satellite observations (such as the standardised precipitation index, standardised precipitation-evapotranspiration index, vegetation condition index, and related streamflow measures) were found to be both practical and sufficiently sensitive to regional drought patterns. However, the review highlighted technical challenges, such as computational difficulties arising from missing values in input data, as well as the absence of reliable groundwater records that would be used for validation.

The study emphasised that no single index can fully represent all drought dimensions, advocating for a multivariate approach that combines complementary indicators. Additionally, it directly informed this project's selection of data-efficient, complementary, and reproducible drought indices as inputs for the DNBC model~\cite{za_drought_review}.

\subsection{Principal Component Analysis for Composite Drought Indicators}
The following study, by Poudel et al., developed a composite drought indicator in the context of New Mexico (CDI-NM) using principal component analysis (PCA) to integrate multiple satellite-derived variables, including rainfall, land surface temperature, and vegetation indices. The aim was to construct a data-driven tool capable of identifying historical drought events and assessing drought extent across the state.

Using datasets from 2003--2019, the authors applied PCA on a monthly basis and validated the suitability of the extracted components using standard checks for sampling adequacy and variable interdependence (using the Kaiser-Meyer-Olkin and Bartlett tests). Model performance was evaluated against the SPI and agricultural yields. The CDI-NM correlated strongly with both these measures, effectively capturing known drought periods between 2003 and 2018.

However, the study’s methodology exposes several recurring weaknesses of PCA-based composite indicators. The technique assumes linearity and temporal stationarity, which constrains its capacity to capture dynamic, non-linear dependencies between input variables. Its reliance on fixed weighting structures further limits adaptability to evolving drought conditions. Additionally, redundancy among vegetation indices and sensitivity to scaling introduce potential distortions in the resulting indicator. These limitations highlight a core shortcoming of dimensionality reduction methods---they capture covariation, not causal relationships. 

This gap directly motivates the use of PGMs, as they provide a more interpretable framework capable of capturing the causal interdependencies and temporal dynamics among input variables using DNBCs~\cite{atmos16070818}.

\subsection{Conclusion}
The reviewed literature illustrates the growing movement toward integrated, probabilistic approaches for drought monitoring. The first two studies established the methodological foundation for DNBC models, demonstrating their capacity to integrate multiple drought indicators while handling uncertainty and variable interdependencies. The third study provided crucial context with regards to which indices are feasible to obtain in the South African context. Finally, the fourth study revealed the limitations of conventional statistical integration methods such as PCA, which rely on linear and static relationships, and highlighted the need for models that can explicitly represent conditional dependencies and temporal dynamics.

These studies and reviews show a clear trajectory from static, correlation-based methods toward dynamic, probabilistic frameworks for drought monitoring. Although South Korean research offers a strong blueprint, the scarcity of composite indicator development in South Africa reveals a significant research gap. This project seeks to bridge this gap by tailoring a DNBC to South Africa. 

\section{Probabilistic Graphical Model Background}
\label{sec:pgm_theory}
\textit{Probabilistic graphical models (PGMs)} provide a framework that sits at the intersection of probability theory and graph theory. In a PGM, a graph structure compactly encodes a complex probability distribution, where nodes represent random variables (RVs) and edges correspond to direct probabilistic interactions between them. This paradigm enables the effective construction and application of models for complex probabilistic reasoning~\cite{Barber_2012, koller_textbook}.

\subsection{Factors and Bayesian Networks}
One way to view a graphical model is as a structured factorisation of a large joint probability distribution. Rather than representing the probability of every possible assignment to all variables in the domain, the model “breaks up” this high-dimensional space into a collection of smaller components---these are called \emph{factors}. A factor is a function defined over one or more RVs that assigns a numerical score to each possible combination of their values. This score reflects how likely that combination is and is thus analogous to a standard probability distribution. However, the scores of a factor need not sum to one, when they do they are considered normalised and become proper probability distributions. By combining a set of locally defined factors according to the graph structure, the global joint distribution over all variables can be reconstructed while maintaining a compact and interpretable representation.


A common and powerful class of PGMs is the \emph{Bayesian network (BN)}, where the graph is directed and acyclic. Each directed edge represents a conditional dependence of a child node given its parent nodes. The network can be viewed as a collection of factors, where each factor corresponds to the conditional probability distribution of a variable given its parents. For example, in a simple three-node network: $A \rightarrow B \rightarrow C$, the joint probability distribution is constructed by multiplying all the factors in the graph as seen in Equation~\eqref{eqn:joint_distr_ex}.
\begin{equation}
    p(A, B, C) = p(A)\,p(B \mid A)\,p(C \mid B)
    \label{eqn:joint_distr_ex}
\end{equation}
This decomposition shows how the overall model is built from locally defined factors, yielding a compact and interpretable representation. Once this joint distribution is specified, any probabilistic query about the system can be answered through inference, as discussed next.

Once the joint distribution is defined, \emph{inference} can be perfomed---that is, determine the probability of any RV or set of RVs given the observations of others. This can be achieved through marginalisation, where irrelevant RVs are summed out. Using the previous example, computing $p(C \mid A)$ is achieved by marginalising over $B$ and applying Bayes' rule as shown below.
\begin{align*} 
    \frac{\sum_B p(A,B,C)}{p(A)}  &= \frac{p(A,C)}{p(A)} \\
    &= p(C \mid A)
\end{align*}
This process allows exact probabilistic reasoning without exhaustively enumerating all combinations of RV values.

\subsection{Efficient Inference with Junction Tree Algorithm}
\label{sec:jt_theory}
For models of even moderate size, directly computing the joint distribution by multiplying all factors becomes intractable---that is, computationally too complex to solve in a reasonable amount of time. In such cases, exact inference can be achieved using the \emph{junction tree (JT)} algorithm. 

This method reorganises the BN into a specific data structure called a junction tree, sometimes called a clique tree, where each node represents a cluster, or clique, of RVs ($\psi_i(\vec{A})$). Any two clusters ($\psi_i(\vec{A})$ and $\psi_j(\vec{B})$) are connected such that the scope of each edge, known as a sepset, consists of the RVs shared between the two adjacent clusters ($\mu_{ij}(\vec{A} \cap \vec{B})$). 

Any valid JT must satisfy two key properties: the \textit{family preservation property}, which ensures that each factor in the original network is assigned to a cluster that contains its scope, and the \textit{running intersection property}, which guarantees that for any RV present in two clusters, all clusters along the unique path between them must also contain that RV~\cite{koller_textbook}.

The inference process involves a coordinated scheme of message-passing, where adjacent clusters in the tree communicate with each other. This communication is achieved by passing \textit{messages}---functions that encapsulate the relevant probabilistic information or, more conceptually, the ``belief''  from one cluster to another. The purpose of this is to allow each cluster to update its local beliefs in a way that is globally consistent across the whole tree. 

Messages are passed according to a specific schedule which begins at the leaf clusters of the tree and sends messages inward. A cluster will only pass a message to its neighbour once it has received messages from all its other neighbours. This ensures that information is propagated correctly, ultimately leading to a state of global consistency known as \textit{calibration}. Once the tree is calibrated, the updated cluster potentials and sepsets represent the true marginal posterior distributions, enabling exact, tractable inference~\cite{lauritzen1988local,koller_textbook,Barber_2012}. 

\subsection{Parameter Estimation with Expectation-Maximisation}
\label{sec:em_theory}
Finally, when the parameters of the model, such as the distributions governing its factors, are unknown, parameter estimation is required. The overarching goal of parameter estimation is typically to maximise the data log-likelihood and several approaches exist for this purpose, including maximum likelihood estimation (MLE), Bayesian inference, variational methods, or gradient-based optimisation techniques. The choice of approach depends on factors such as the structure of the model, the presence of latent RVs, and computational constraints.

When models involve latent RVs (these are RVs that are never observed) or incomplete data, direct optimisation of the likelihood becomes intractable. Although gradient-based methods are applicable, they are computationally expensive. The expectation-maximisation (EM) algorithm is frequently preferred in the context of PGMs, as it is specifically designed for likelihood optimisation with latent RVs. EM provides a stable, iterative framework that yields improvements at each step. The algorithm proceeds by alternating between two core steps:
\begin{enumerate}[label=(\roman*)]
    \item The expectation or E-step, which computes the expected values of the latent variables given the observed data and current parameter estimates.
    \item The maximisation or M-step, treats the calculated expected sufficient statistics as observed, and performs maximum likelihood estimation to derive a new set of parameters.
\end{enumerate}
This process repeats until convergence, yielding parameter estimates consistent with both the observed data and the probabilistic structure of the model~\cite{koller_textbook,moon_tk}.

Together, these concepts form the theoretical basis for the DNBC proposed in this study, which combines temporal dependence with probabilistic reasoning to model drought conditions.
