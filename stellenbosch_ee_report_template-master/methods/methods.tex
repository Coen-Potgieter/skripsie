\graphicspath{{methods/fig/}}
\chapter{Methodology}
\label{chap:methods}

This chapter describes the methodology developed to build and apply the dynamic naive Bayes classifier (DNBC) for composite drought monitoring in the the southwestern Cape region. The primary objective was to create a model that integrates meteorological, hydrological, and agricultural drought indices into a unified, interpretable measure of drought conditions.

The process began with data acquisition and preprocessing, where the necessary input variables were sourced, cleaned, and transformed into discrete drought indices. Subsequently, the core DNBC model was developed, specifying its probabilistic structure and the procedures for inference and parameter learning. Finally, the model was implemented using appropriate computational tools and algorithms.

\section{Data and Index Construction}
\label{sec:data_and_indices}
\subsection{Overview of Input Indices}

The development of a composite drought indicator requires careful selection of input variables that capture the different aspects of drought. Three indices were selected, the first being the \textit{standardised precipitation index (SPI)} which measures how much precipitation deviates from the long-term average making it a useful statistical measure for meteorological drought~\cite{spi_seminal_paper}. The second chosen index is the \textit{streamflow drought index (SDI)}. Similar to the SPI, it measures how much river streamflow deviates from long-term averages and thus helps quantify hydrological drought~\cite{sdi_seminal_paper}. Finally, the \textit{normalised difference vegetation index (NDVI)} is a remote-sensing indicator widely used to monitor vegetation health and stress. This serves as a proxy for agricultural drought~\cite{ndvi_seminal_paper}. These indices were chosen based on their widespread use in literature and the availability of data. Data scarcity is a challenge in South Africa, as openly accessible, long and consistent drought-related datasets are limited. Consequently, the choice of indices attempts to strike a balance between theory and pragmatic constraints~\cite{za_drought_review,za_drought_review2,dnbc_drought_first,dnbc_drought_second}. For a summary of these input indices, see Table~\ref{tbl:drought_indices}.

\begin{table}[h!]
    \centering
    \footnotesize
    \caption{Selected drought indices and their characteristics.}
    \begin{tabular}{>{\centering\arraybackslash}m{0.2\linewidth} >{\centering\arraybackslash}m{0.15\linewidth} >{\centering\arraybackslash}m{0.25\linewidth} >{\centering\arraybackslash}m{0.25\linewidth}}
        \toprule
        \textbf{Name} & \textbf{Acronym} & \textbf{Drought Aspect} & \textbf{Primary Data} \\
        \toprule
        Standardised precipitation index & SPI & Meteorological & Precipitation \\
        \midrule
        Streamflow drought index & SDI & Hydrological & River streamflow \\
        \midrule
        Normalised difference vegetation index & NDVI & Agricultural & Remote-sensing \\
        \bottomrule
    \end{tabular}
    \label{tbl:drought_indices}
\end{table}

\subsection{Data Sources}
\label{sec:data_sources}
Monthly precipitation data was obtained from an open source dataset authored by Conradie et al. and maintained by the University of Cape Town (UCT)~\cite{uct_data}. This dataset provides rainfall values captured at weather stations across the southwestern region of South Africa and spans the period 1979--2019 in CSV format. This data offers consistency and a high degree of granularity.

Streamflow records were sourced from a collection of river gauging stations managed by the Department of Water and Sanitation (DWS), which maintains audited historical hydrology data~\cite{DWS_2011}. The dataset provides measurements from individual locations across the country, with records spanning periods anywhere between 1903 and 2025. The daily data, accessible only in a text format on the DWS website, was scraped and converted to a structured CSV format.

NDVI was not computed but rather obtained directly from the ``NOAA Climate Data Record (CDR) of AVHRR Normalised Difference Vegetation Index (NDVI), Version~5'' dataset~\cite{ndvi_data}. This dataset offers daily records that span the period 1981--2025, covers the entire globe in rasterised grids, and is provided in NetCDF format. 

With the three primary data sources (precipitation, streamflow, and satellite-derived vegetation indices) acquired and converted into consistent, usable formats, the foundational data preparation is complete. The subsequent phase involves preprocessing this dataset for input into the dynamic naive Bayes classifier (DNBC) model.

\subsection{Preprocessing and Cleaning}
\label{sec:preprocessing}
The first step in preprocessing was to establish the study period as well as the temporal resolution. The study was constrained to the overlapping period of the three data sources, which spans 1981--2019. Since the DNBC works on monthly time steps, all the data was processed to a monthly resolution to ensure temporal consistency across indices.

The preprocessing of the SPI and SDI data sources was largely similar, with a few key distinctions. Both rainfall and streamflow data sources were subjected to the following station filtering rules:
\begin{itemize}
    \item Stations that did not span the full 1981--2019 study period were removed.
    \item Stations containing any missing values within this period were removed.
    \item Outlier detection was applied using the interquartile range (IQR) rule. This meant removing any stations that had a record which deviated by more than $1.5\times IQR$ from the median of surrounding stations for that time step.
\end{itemize}

Although this criterion seems strict, the high density of available stations allowed for 205 weather stations and 154 river gauging stations to be left remaining. These clean stations collectively define the study area and are shown in Figure~\ref{fig:study-area}. Finally, the daily streamflow data was aggregated to monthly time steps, aligning with the temporal resolution of the raw precipitation data.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{study-area.png}
    \caption[Map of Stations in Study Area]{Locations of weather (blue) and river gauging (red) stations used in this study.}
    \label{fig:study-area}
\end{figure}

The preprocessing workflow for the NDVI data was comparatively simple. It required subsetting the global dataset to the established study area and defined study period, recalibrating the scaled integers to the standard floating-point range ($-1.0$ to $1.0$), and aggregating the daily data to monthly averages. The results were then exported from NetCDF to CSV to align with the data format of the other indices.

\subsection{Index Computation}
\label{sec:index-calc}
With all datasets now cleaned, temporally aligned, and aggregated to monthly resolution, the next step involved computing the drought indices. No further computation was required for NDVI following the preprocessing stage. This subsection therefore focuses on the computation of the SPI and SDI.

\subsubsection{Theoretical Background}

Both SPI and SDI are statistical measures that transform raw observations into standardised indicators of drought intensity~\cite{spi_seminal_paper, sdi_seminal_paper}. The procedure for each index is conceptually similar and can be summarised as follows:

\textbf{1. Window-Based Aggregation:} Observations are first aggregated over a specified temporal window to capture drought persistence at different timescales. Common practice varies between 3-, 6-, 9-, or 12-month windows, each reflecting short to long term drought characteristics. 

\textbf{2. Distribution Fitting:} For each station and calendar month, the aggregated series is assumed to follow a parametric probability distribution. This accounts for the seasonality present in both rainfall and streamflow data. The fitted distribution is then used to compute the cumulative probability of each observed value, which expresses the rarity of that observation relative to values from the same calendar month in other years. In other words, it quantifies how far current conditions deviate from the historical norm for that time of year. Various distributions has been proposed in the literature, including gamma, Weibull, and log-normal~\cite{za_drought_review}.

\textbf{3. Standard Normal Transformation:} Once the cumulative probability $p(x)$ of an observation is obtained, it is transformed into a standard normal distribution ($Z$-score). This produces continuous, standardised values with mean zero and unit variance. Negative values indicate conditions that are drier than usual, while positive values represent wetter than usual. The resulting $Z$-scores form the SPI or SDI, depending on the underlying input data (precipitation or streamflow).

\subsubsection{Implementation in This Study}
In this study, both the SPI and SDI were computed independently for each station using the gamma distribution, keeping consistent with standard practice in drought monitoring literature~\cite{za_drought_review}. While most studies employ fixed aggregation windows, this project treated the aggregation window length as a hyperparameter that was determined during model selection. 

\subsection{Discretisation of Indices}
Once again, all three input indices---SPI, SDI, and NDVI---are all continuous measures. However, the proposed model requires discrete inputs. Accordingly, each index was discretised into categorical bins based on thresholds widely used in literature. This discretisation not only aids in model implementation but it also motivates interpretability. Table~\ref{tbl:discretisation} below illustrates the bins used.

\begin{table}[H]
    \centering
    \caption{Discretisation thresholds for drought indices.}
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{lcc}
            \toprule
            \textbf{Category} & \textbf{SPI / SDI}  \\
            \midrule
            Severe drought    & $\leq -1.5$        \\
            Moderate drought  & $-1.5 < x \leq -0.5$ \\
            Normal            & $-0.5 < x < 0.5$  \\
            Moderate wet      & $0.5 \leq x < 1.5$ \\
            Severe wet        & $\geq 1.5$        \\
            \bottomrule
        \end{tabular}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \begin{tabular}{lcc}
            \toprule
            \textbf{Category} & \textbf{NDVI} \\
            \midrule
            Bare soil / water    & $-1 < x <  0.1$   \\
            Sparse vegetation  & $0.1 \leq x < 0.2 $\\
            Moderate vegetation            & $0.2 < x < 0.4$    \\
            Dense vegetation      & $0.4 \leq x < 0.6$ \\
            High density vegetation        & $ 0.6 \leq x < 1 $          \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \label{tbl:discretisation}
\end{table}

At this stage, all three drought indices were computed, discretised, and temporally aligned over the 1981--2019 period. These indices together formed the input set for the DNBC. 

\section{Model Development}
\label{sec:model_development}
This section details the development of the DNBC for this project. The model development process proceeds through five key stages: design, where the model is fully specified; inference, where algorithms for answering queries about the model are formalised; parameter estimation, where model parameters are learned from data; model selection, where various hyperparameter configurations are evaluated to identify the most representative model; and finally, output decoding, where the model results are exported in two forms. It is important to note that this section does not introduce new theoretical contributions but rather directly implements established DNBC theory as found in existing literature~\cite{dnbc_drought_first,dnbc_drought_second}.

\subsection{Model Design}
\label{sec:model_design}
\subsubsection{Defining the Random Variables}
The proposed DNBC is constructed with $3$ input random variables (RVs) observed across $T$ discrete time steps. All RVs in the model are treated as discrete. The first set of RVs corresponds to the latent drought states at each time step, denoted by:
\[
S_t \in \{1,2,\dots,m\}, \quad t = 1, \dots, T,
\]
where $m$ represents the number of possible drought states. This value of $m$ is not fixed, but will rather be determined via model selection. The integer values of $S_t$ are ordinally scaled, representing a spectrum of drought severity from severe drought (lower values) to wet conditions (higher values), with the central values typically indicating a neutral or normal state.

The second set of RVs corresponds to the observed input indices, denoted by
\[
A_t^{(n)} \in \{1,2,\dots,5\}, \quad n = 1, 2, 3, \quad t = 1, \dots, T.
\]
The meanings attached to each value follows directly from the discretisation shown above in Table~\ref{tbl:discretisation}. For clarity, below is the assignment of these input indices which constitute the dataset $\mathcal{D}$.
\[
\text{SPI} = A_t^{(1)}, \quad 
\text{SDI} = A_t^{(2)}, \quad 
\text{NDVI} = A_t^{(3)}.
\]

We define the following notation which will be used throughout the model formulation:
\[
    \vec{S}_{1:T} = (S_1, S_2, \dots, S_T), \quad 
    \vec{A}_{1:T} = (\vec{A}_1, \vec{A}_2, \dots, \vec{A}_T),
\]
where each $\vec{A}_t = \{A_t^{(1)}, A_t^{(2)}, A_t^{(3)}\}$.  

\subsubsection{Graphical Structure and Joint Distribution}
Figure~\ref{fig:dnbc-diagram} below displays the model diagram for the DNBC model as a Bayesian network (BN) with $T$ time steps and $3$ input RVs at each time step.  

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{dnbc-diagram.png}
    \caption[DNBC Model Diagram]{
        The DNBC can be represented as a Bayesian network (BN) unfolding over time. At each time step $t$, a latent drought state $S_t$ is modelled as a discrete random variable that governs the latent structure, while the observed input variables $\vec{A}_t = \{A_t^{(1)}, A_t^{(2)}, A_t^{(3)}\}$ are each solely dependent on $S_t$}
    \label{fig:dnbc-diagram}
\end{figure}

As discussed in Section~\ref{sec:pgm_theory}, we can utilise this graph to construct the joint probability distribution which is shown in Equation~\eqref{eqn:joint_distr_dnbc} below.
\begin{equation}
    p(\vec{S}_{1:T}, \vec{A}_{1:T}) = p(S_1) \cdot \prod\limits_{t=1}^{T-1} p(S_{t+1} \mid S_t) \cdot \prod\limits_{n=1}^{3} \prod\limits_{t=1}^T p(A^{(n)}_t \mid S_t)
    \label{eqn:joint_distr_dnbc}
\end{equation}

\subsubsection{Parameterising the Model}
Following established literature, the DNBC is fully specified by three sets of parameters, namely the prior, transition, and emission probabilities.

\paragraph{Prior Probabilities:}
The prior probabilities, denoted as $\vec{\theta}_1$, represents the initial belief over the latent drought states before any observations are made. This distribution captures the likelihood of the system starting in each possible state at time $t=1$. Formally, the prior probability for state $i$ is defined as:
\[
\pi_i \equiv p(S_1 = i)
\]
where $\sum_{i=1}^{m} \pi_i = 1$. These parameters are captured in the factor table shown in Table~\ref{tbl:priors_factor_table}.

\begin{table}[!h]
    \mytable
    \caption[Priors Factor Table]{Factor table for the prior probabilities ($\vec{\theta}_1$)}
    \begin{array}{c | c}
        S_1 & p(S_1) \\ 
        \hline
        1 & \pi_1 \\ 
        2 & \pi_2 \\ 
        \vdots & \vdots \\
        m & \pi_m \\ 
    \end{array} 
    \label{tbl:priors_factor_table}
\end{table}

\paragraph{Transition Probabilities:}  
Denoted as $\vec{\theta}_2$, this set of parameters define the likelihood of the system moving from one latent drought state to another between consecutive time steps. Formally, the probability of transitioning from state $i$ at time $t$ to state $j$ at time $t+1$ is denoted as:
\[
a_{i,j} \equiv p(S_{t+1} = j \mid S_t = i).
\]
where $\sum_{j=1}^m a_{i,j} = 1$ for all $i$. These parameters determine the model's temporal dynamics and can be presented both as a factor table and a transition matrix, shown in Table~\ref{tbl:transition_factor_table}.

\begin{table}[!h]
    \mytable
    \caption[Transition Factor Table and Transition Matrix]{Factor table for the transition probabilities ($\vec{\theta}_2$) and transition matrix $\vec{P}^1$.}
        \begin{array}{ccc}
            \begin{array}{c c | c}
                S_t & S_{t+1} & p(S_{t+1} \mid S_t) \\ 
                \hline
                1 & 1  & a_{1,1} \\ 
                1 & 2  & a_{1,2} \\ 
                \vdots & \vdots  & \vdots \\
                1 & m  & a_{1, m} \\ 
                2 & 1  & a_{2, 1} \\ 
                2 & 2  & a_{2, 2} \\ 
                \vdots & \vdots  & \vdots \\
                m & m  & a_{m,m} \\ 
            \end{array} 
            &
            \equiv
            &
            \vec{P}^1 = 
            \begin{bmatrix}
                a_{1,1} & a_{1,2} & \dots & a_{1,m} \\
                a_{2,1} & a_{2,2} & \dots & a_{2,m} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{m,1} & a_{m,2} & \dots & a_{m,m} \\
            \end{bmatrix}
        \end{array} 
    \label{tbl:transition_factor_table}
\end{table}

\paragraph{Emission Probabilities:}  
The relationship between the latent drought states and the observed drought indices at each time step is defined by the emission probabilities, denoted as $\vec{\theta}_3$. These parameters quantify the likelihood of observing a specific value for a given input index (SPI, SDI, or NDVI) when the system is in a particular hidden drought state. Formally, the probability of observing value $j$ for the $n$-th input RV at time $t$, given the latent state $i$, is defined as:
\[
b_i^{(n)}(j) \equiv p(A_t^{(n)} = j \mid S_t = i).
\]

This set of conditional probabilities is presented as a factor table, shown below in Table~\ref{tbl:emission_factor_table}.
\begin{table}[!h]
    \mytable
    \caption[Emission Factor Table]{Factor table for the emission probabilities ($\vec{\theta}_3$)}
        \begin{array}{c c | c}
            A^{(n)}_t & S_t & p(A^{(n)}_t \mid S_t) \\ 
            \hline
            1 & 1  & b_1^{(n)}(1) \\ 
            1 & 2  & b_2^{(n)}(1) \\ 
            \vdots & \vdots  & \vdots \\
            1 & m  & b_m^{(n)}(1) \\ 
            2 & 1  & b_1^{(n)}(2) \\ 
            2 & 2  & b_2^{(n)}(2) \\ 
            \vdots & \vdots  & \vdots \\
            5 & m  & b_m^{(n)}(5) \\ 
        \end{array} 
    \label{tbl:emission_factor_table}
\end{table}

The full set of these parameters, $\Theta = (\vec{\theta}_1, \vec{\theta}_2, \vec{\theta}_3)$, fully determine the complete DNBC. A key characteristic of this model is that the parameters are time-invariant, meaning the probability of transitioning between drought states and the likelihood of observing specific index values remain constant irrespective of the time step $t$. This is one of the model's core assumptions, which is discussed in the following section.

\subsubsection{Assumptions}
Finally, it is important to note the three assumptions the DNBC is built on, which defines its structure but also impose inherent limitations:
\begin{enumerate}[label=(\roman*)]
    \item The dynamic process of the state sequence $S_t$ follows a first-order Markov chain. This means the state at time $t+1$ is conditionally dependent only on the state at time $t$. \label{item:assumption_1}
    \item The dynamic process is stationary, implying that the transition probabilities between states are constant over the entire time series. \label{item:assumption_2}
    \item For each time step $t$, the model assumes conditional independence among the input variables $\vec{A}_t$ given the corresponding hidden drought state $S_t$. \label{item:assumption_3}
\end{enumerate}

These assumptions have important practical implications. The first-order Markov assumption~\ref{item:assumption_1} implies that the model has a very short ``memory'', only using the information from the previous time step to determine the current one. Stationarity~\ref{item:assumption_2} simplifies the model by requiring only a single set of transition and emission probabilities for the entire period, as seen in the previous section. Most critically, the conditional independence assumption~\ref{item:assumption_3} means that if the true, underlying drought state $S_t$ is known, then knowing the value of one indicator (for example the SPI) provides no additional information about the value of another (such as the SDI or NDVI). In other words, the latent state $S_t$ fully accounts for all the dependencies between the observed indices. Additionally, this third assumption allows for the following factorisation which will become useful at a later stage.
\begin{equation} 
    \begin{align} 
        p(\vec{A}_{t} \mid S_t) &= p(A_t^{(1)}, A_t^{(2)}, A_t^{(3)} \mid S_t) \\ 
        &= p(A_t^{(1)} \mid S_t)p(A_t^{(2)} \mid S_t)\dotsp(A_t^{(3)} \mid S_t) \\ 
        &= \prod\limits_{n=1}^3 p(A_t^{(n)} \mid S_t) 
    \end{align} 
    \label{eqn:attribute_rv_factorisation} 
\end{equation}

\subsection{Inference}
\label{sec:inference}

In this section, inference for the DNBC is developed under the assumption that the parameters $\Theta$ are known and the input variables $\vec{A}_{1:T}$ are observed. Since these variables are not random at this stage, the task becomes trying to infer the distribution of the hidden drought states:
\[
    p(\vec{S}_{1:T} \mid \vec{A}_{1:T}, \Theta).
\]
This will later be used for the E-step in the expectation-maximisation (EM) algorithm.

The inference procedure is carried out using the junction tree (JT) algorithm which provides a means to obtain exact inference efficiently. The first step is to construct a JT that adheres to both the family preservation property as well as the running intersection property illustrated in Figure~\ref{fig:jt_diagram}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{jt-diagram.png}
    \caption[Junction Tree Diagram]{Junction tree representation of the DNBC. Each cluster groups together latent state variables and observed input indices, with sepsets defined along the edges. Messages are propagated through the tree to perform exact inference.}
    \label{fig:jt_diagram}
\end{figure}

Each cluster in the tree represents a subset of RVs from the original DNBC. The relationship between these variables within a cluster is defined by its cluster potential, shown in Table~\ref{tbl:cluster_potentials}. This structure relies on the factorisation of the observed indices given in Equation~\eqref{eqn:attribute_rv_factorisation}.
\begin{table}[!h]
    \mytable
    \caption[Cluster Potentials of Junction Tree]{Cluster potentials for the junction tree of the proposed model.}
    \begin{aligned}[c]
        &- \\
        \psi_2(S_1, S_2) &= p(S_2 \mid S_1) \\
        &\vdots \\
        \psi_t(S_{t-1}, S_t) &= p(S_t \mid S_{t-1}) \\
        &\vdots \\
        \psi_T(S_{T-1}, S_T) &= p(S_T \mid S_{T-1}) \\
    \end{aligned}
    \qquad \qquad \qquad
    \begin{aligned}[c]
        \psi_1(S_1, \vec{A}_1) &= p(S_1)p(\vec{A}_1 \mid S_1) \\
        \psi_2(S_2, \vec{A}_2) &= p(\vec{A}_2 \mid S_2) \\
        &\vdots \\
        \psi_t(S_t, \vec{A}_t) &= p(\vec{A}_t \mid S_t) \\
        &\vdots \\
        \psi_T(S_T, \vec{A}_T) &= p(\vec{A}_T \mid S_T) \\
    \end{aligned}
    \label{tbl:cluster_potentials}
\end{table}


\subsubsection{Message-Passing}

Once the JT is defined, the next step is message-passing. Recall from Section~\ref{sec:jt_theory} that, conceptually, a message represents the ``belief'' of one cluster about the variables it shares with another. As illustrated in Figure~\ref{fig:jt_diagram}, messages are passed directionally between clusters and are defined over the scope of their sepset. 

For message-passing, either the belief update (BU) or belief propagation (BP) approach may be followed. While both are equivalent in terms of computation within the JT algorithm, BP was used because it is conceptually simpler. 

Since the objective is to compute the posterior distribution $p(\vec{S}_{1:T} \mid \vec{A}_{1:T}, \Theta)$, only the clusters $\psi_t(S_t, S_{t+1})$ and their corresponding sepsets $\mu_{t,t+1}(S_t)$ need to be fully calibrated. Thus, downward messages---those from $\psi_t(S_{t-1}, S_t)$ to $\psi_t(S_t, \vec{A}_t)$---are not required for this task. Message definitions are therefore restricted to those shown in Figure~\ref{fig:jt_diagram}, which are sufficient for obtaining the desired marginals.

Full theoretical derivations are omitted here for brevity, as the necessary conceptual foundation has already been provided. The following therefore presents only the defined messages used in this study.

\paragraph{Upward messages:}  
Because the observed attribute variables $\vec{A}_t$ are no longer random, the marginalisation over their possible values becomes redundant. The upward messages therefore collapse to likelihood terms that express how compatible the observed evidence is with each latent state $S_t$. This yields the following simplified form:
\begin{align*} 
    \delta_{t\uparrow} (S_t) &= \sum\limits_{\vec{A}_t} \psi_t(S_t, \vec{A}_t) \\ 
    &= \sum\limits_{\vec{A}_t} p(\vec{A}_t \mid S_t) \\ &= p(\vec{A}_t \mid S_t) \numberthis \label{eqn:updward_message} 
\end{align*}

\paragraph{Rightward messages:}  
Once again, In the JT algorithm message propagation begins at the leaf clusters and proceeds inward. Accordingly, rightward messages originate at the leftmost cluster. The initial rightward message, Equation~\eqref{eqn:rightward_message_init}, reflects the joint contribution of the prior distribution on $S_1$ and its associated observation likelihood. Thereafter, messages are recursively propagated through time as shown in Equation~\eqref{eqn:rightward_message_final}. This process effectively integrates information from previous states and the current observation.


\begin{align*} 
    \delta_{1 \rightarrow 2} (S_1) &= \sum\limits_{\vec{A}_1} \psi_1(S_1, \vec{A}_1) \\ 
    &= \sum\limits_{\vec{A}_1} p(S_1)p(\vec{A}_1 \mid S_1) \\ 
    &= p(S_1)p(\vec{A}_1 \mid S_1) \numberthis \label{eqn:rightward_message_init} \\ 
    \\ 
    \delta_{t \rightarrow t+1} (S_t) &= \sum\limits_{S_{t-1}} \psi_t(S_{t-1}, S_t) \delta_{t-1 \rightarrow t}(S_{t-1}) \delta_{t\uparrow}(S_t) \\ 
    &= \sum\limits_{S_{t-1}} p(S_t \mid S_{t-1}) \delta_{{t-1} \rightarrow t}(S_{t-1}) p(\vec{A}_t \mid S_t) \\ 
    &= p(\vec{A}_t \mid S_t) \sum\limits_{S_{t-1}} p(S_t \mid S_{t-1}) \delta_{{t-1} \rightarrow t}(S_{t-1}) \numberthis \label{eqn:rightward_message_final} 
\end{align*}

\paragraph{Leftward messages:}  
Similarly, leftward propagation begins at the final cluster and proceeds backward through time. The initial message from the last time step, Equation~\eqref{eqn:leftward_message_init}, combines the transition and emission probabilities at $T$, while the recursive form, Equation~\eqref{eqn:leftward_message_final}, propagates backward through time in the same way that rightward messages move forward. 
\begin{align*} 
    \delta_{T-1 \leftarrow T} (S_{T-1}) &= \sum\limits_{S_T} p(S_T \mid S_{T-1})p(\vec{A}_T \mid S_T), \numberthis \label{eqn:leftward_message_init} \\ 
    \delta_{t-1 \leftarrow t} (S_{t-1}) &= \sum\limits_{S_t} p(S_t \mid S_{t-1}) \delta_{t \leftarrow t+1}(S_t) p(\vec{A}_t \mid S_t). \numberthis \label{eqn:leftward_message_final} 
\end{align*}

Together, These three sets of messages---upward, rightward and leftward---when passed in the correct ordering yield a fully calibrated JT. At calibration, the cluster potentials and sepsets represent the true marginal distributions: $\psi_t(S_{t-1}, S_t) = p(S_{t-1}, S_t)$ and $\mu_{t, t+1}(S_t) = p(S_t)$. This provides tractable computation of the target posterior distribution $p(\vec{S}_{1:T} \mid \vec{A}_{1:T}, \Theta)$.

\subsubsection{Forward-Backward Equivalence}
At this stage it is natural to note that the procedure described above is a generalisation of the classical \textit{forward-backward} algorithm for hidden Markov models (HMMs), which dominate the literature~\cite{dnbc_drought_first,dnbc_drought_second}. The inward and outward messages of the JT formulation are algebraically equivalent to the forward and backward recursions respectively~\cite{binder1997space,aviles,hmm_slides}. The distinction lies solely in formulation where the JT framework provides a general inference architecture for arbitrary graphical models and the forward-backward algorithm is the special case applied to chain-structured models such as HMMs and DNBCs.

As a result of this equivalence, the rightward messages of the JT correspond to the forward quantities $\alpha_t = p(\vec{A}_{1:t}, S_t)$, while the leftward messages correspond to the backward quantities $\beta_t = p(\vec{A}_{t+1:T} \mid S_t)$. When these messages are combined at a cluster or sepset, the resulting marginal distributions $p(S_t \mid \vec{A}_{1:T})$ and $p(S_t, S_{t+1} \mid \vec{A}_{1:T})$ are identical to those obtained through the forward-backward algorithm.

\paragraph{Connection to Baum-Welch:}  
This parallel extends directly to the \textit{Baum-Welch} algorithm~\cite{wiki:baum_welch}, which is the EM implementation for parameter learning in HMMs. In Baum-Welch, the E-step is performed using the forward-backward quantities, and the M-step updates the transition and emission probabilities to maximise the expected log-likelihood. The JT formulation highlights the structural perspective, while forward-backward and Baum-Welch remain the traditional algorithms in the literature. Both views are mathematically equivalent and lead to the same computations.

This link provides a smooth transition to the following subsection, where parameter estimation for the model is discussed using the EM framework paired with the JT algorithm.


\subsection{Parameter Estimation}
\label{sec:param_estimation}

The next step involves estimating the parameters which is required to align the model with the observed data for subsequent inference. Because the DNBC includes latent RVs that are never observed, the expectation-maximisation (EM) algorithm was used. Again, the following subsection details the application of established literature and theory revolving PGMs~\cite{koller_textbook,moon_tk}. As discussed in Section~\ref{sec:em_theory}, EM provides an efficient iterative procedure that alternates between computing the expected sufficient statistics of the latent states (E-step) and updating parameter estimates to maximise the data log-likelihood (M-step). This approach is particularly suited to models such as the DNBC, where the latent state sequence must be inferred jointly with the model parameters.

It is necessary, before detailing the EM steps, to establish the following notation which consolidates terms from previous sections. Firstly being the hidden states, secondly the observed RVs or the dataset of the problem, and lastly the full set of model parameters (the prior, transition, and emission probabilities). This notation is shown below.
\begin{align*} 
    \mathcal{H} &= \vec{S}_{1:T} = (S_1,S_2,\dots, S_T) \\ 
    \mathcal{D} &= \vec{A}_{1:T} = (\vec{A}_1, \vec{A}_2, \dots, \vec{A}_T) \\ 
    \Theta &= (\vec{\theta}_1, \vec{\theta}_2, \vec{\theta}_3)
\end{align*}

\paragraph{1. E-Step:} In this step, the current parameter estimates $\Theta$ are held fixed while we compute the posterior distribution over the latent state sequence:
\begin{equation}
    q(\mathcal{H}) = p(\mathcal{H} \mid \mathcal{D}, \, \Theta) = p(\vec{S}_{1:T} \mid \vec{A}_{1:T}, \, \Theta).
    \label{eqn:e_step}
\end{equation}

This corresponds directly to the inference problem discussed previously in Section~\ref{sec:inference}. The discussed JT algorithm yields the required marginal and pairwise posteriors, namely $q(S_t)$ and $q(S_t, S_{t+1})$, without explicitly computing the full joint distribution in Equation~\eqref{eqn:e_step}. These quantities are sufficient for parameter re-estimation in the following M-step.

\paragraph{2. M-Step:} In the M-step, the choice of distribution $q(\mathcal{H})$ stays fixed to what was determined in the E-step---meaning the posteriors $q(S_t)$ and $q(S_t, S_{t+1})$ remains constant. The parameters $\Theta$ are then updated by maximising the expected complete-data log-likelihood:
\[
    \mathcal{Q}(\Theta) = \sum\limits_{\mathcal{H}} q(\mathcal{H}) \cdot \log \, p(\mathcal{D}, \, \mathcal{H} \mid \Theta).
\]

Expanding this expression reveals contributions from the prior, transition, and emission components of the model. A full derivation of these terms is not the focus here and can be found in standard explanations of the EM algorithm. The key idea to grasp is that the maximisation over $\Theta$ decomposes naturally across its components $\vec{\theta}_1$, $\vec{\theta}_2$, and $\vec{\theta}_3$, leading to the standard re-estimation update rules~\cite{jm3,xing_slides}:
\begin{equation}
    \boxed{\;\pi_i^{\text{new}} = q(S_1=i)\;}
    \label{eqn:prior_update_rule}
\end{equation}
\begin{equation}
    \boxed{\;a_{i,j}^{\text{new}}=\frac{\sum\limits_{t=1}^{T-1} q(S_t=i,S_{t+1}=j)}{\sum\limits_{t=1}^{T-1} q(S_t=i)}\;}
    \label{eqn:transition_update_rule}
\end{equation}
\begin{equation}
    \boxed{\; b_i^{(n)}(j)^{\text{new}} = \frac{\sum\limits_{t=1}^T q(S_t = i) \cdot \mathbf{1}(A_t^{(n)} = j)}{\sum\limits_{t=1}^T q(S_t = i)} \;}
    \label{eqn:emission_update_rule}
\end{equation}


\subsubsection*{Practical Considerations}
The EM algorithm requires an initial distribution over the model parameters to begin the iterative process. Its performance is highly sensitive to these initial conditions, as the algorithm may converge to a local rather than a global optimum. Consequently, the choice of initialisation can substantially influence the quality of the final estimates. Initialisation strategies typically involve random initialisation according to either a uniform or Gaussian distribution~\cite{moon_tk}.

A termination criterion is necessary to prevent overfitting, since the algorithm guarantees improvement of the likelihood with each iteration, even when doing so yields negligible improvements. Behaviourally, the EM algorithm typically performs large updates in early iterations, followed by progressively smaller refinements as posterior distributions stabilise. While EM acts as a useful tool for parameter learning in the DNBC, it remains sensitive to poor initialisation and can also converge slowly.

\subsection{Model Selection}
\label{sec:model_selection}

Model selection is a critical step in ensuring that the DNBC has the best configuration to accurately capture the underlying drought dynamics within the data, while avoiding the threat of overfitting. The goal is to find the most appropriate level of model complexity, determined by the number of latent drought states $S_t$ (denoted by $m$), and to select suitable hyperparameters governing the rolling window sizes of the SPI and SDI input indices. The overarching goal here is to strike a balance between model complexity and goodness of fit.

\subsubsection{Latent-State Cardinality $m$}

As discussed in Section~\ref{sec:overview_design}, the output of each latent RV ($S_t$) can be interpreted along a conceptual scale that is binned into categories. This means that larger values of $m$ allows for finer grained bins and thus gives the model a greater ability to capture subtle drought dynamics. However, this increases the number of free parameters and thus the risk of overfitting. Conversely, a smaller $m$ enforces simplicity but may fail to capture the full complexity of the observed data.

To guide this choice, three widely used criteria were applied: the Akaike information criterion (AIC), the Bayesian information criterion (BIC), and the maximised log-likelihood of the fitted model~\cite{murphy_2012}. The equations for AIC and BIC are given by:
\begin{align*}
    AIC &= -2 \cdot \log L(\Theta) + 2p, \numberthis \label{eqn:aic} \\
    BIC &= -2 \cdot \log L(\Theta) + p \cdot \log k, \numberthis \label{eqn:bic}
\end{align*}
where $L(\Theta)$ is the maximised value of the likelihood function, $p$ is the number of free parameters, and $k$ is the number of data points.

Both AIC and BIC embody the principle of Occam’s razor~\cite{Barber_2012}, which favors models that achieve high likelihood with minimal complexity. BIC applies a stronger penalty on complexity and is thus generally favoured for conservative model selection. This study followed the following framework for selecting $m$:
\begin{enumerate}
\item \textbf{Primary:} Choose the model with the lowest BIC, penalising unnecessary complexity.
\item \textbf{Secondary:} Use AIC to cross-check results.
\item \textbf{Tertiary:} Inspect the log-likelihood curve. If improvements in $\log L(\Theta)$ diminish as $m$ increases, the simpler model is preferred (this is the well known “elbow rule”).
\end{enumerate}

\subsubsection{Rolling-Window Hyperparameters}

The rolling windows applied to the SPI and SDI indices introduce additional hyperparameters for the problem. Each index can be computed over 3-, 6-, 9-, or 12-month windows, and any combination of these can be paired to form the input configuration for the DNBC. Unlike $m$, however, varying the rolling window lengths does not alter the number of free parameters in the model. Thus, criteria such as AIC or BIC are not meaningful here, and only the maximised log-likelihood of the fitted model is used to compare configurations.

Empirically, these two types of hyperparameters, $m$ and rolling-window size combination, are decoupled. The value of $m$ directly controls the capacity of the model, while the window sizes control the temporal aggregation of the input data. Since one affects the input domain and the other the latent structure, their influence on the overall likelihood are independent. In practice, this means the choice for $m$ and the window size combination can be selected in isolation of each other, the former using AIC, BIC and log-likelihoods and the latter using only log-likelihoods. While a quantitative proof of full independence is non-trivial, it is not discussed here. Therefore, the model selection relies solely on this conceptual argument of decoupling.

\subsubsection{Determining $k$ and $p$}

The quantities $k$ and $p$ in Equations~\eqref{eqn:aic}--\eqref{eqn:bic} are defined following common conventions in the literature and implementation libraries such as \texttt{seqHMM}~\cite{cran_hmmSeq,free_params_slides}.

The number of data points $k$ is taken as the total number of number of data points and is calculated as $k = T \times 3$.

The number of free parameters $p$ corresponds to the model’s degrees of freedom. In the DNBC, this includes contributions from the prior, transition, and emission probabilities:
\begin{align*}
    p &= (m-1) + m(m-1) + \sum_{n=1}^3 m(C_n - 1) \\
    &= m^2 - 1 + m \sum_{n=1}^3 (C_n - 1),
\end{align*}
where $C_n$ is the cardinality, or number of categories, of the $n$-th input index.

\subsubsection{Log-Likelihood Estimation}

The log-likelihood, $\ell(\Theta)$, measures the probability of the observed data given the current model parameters:
\[
    \ell(\Theta) = \log p(\vec{A}_{1:T} \mid \Theta).
\]
Obtaining the maximised log-likelihood $L(\Theta)$ from this will be discussed later in Section~\ref{sec:model_implementation2}.

The value of $\ell(\Theta)$ can be obtained directly from the messages computed during inference using the JT algorithm. After completing the message-passing procedure, a single downward message, $\delta_{\downarrow T}(S_T)$, is sent to fully calibrate the final cluster $\psi_T(S_T, \vec{A}_T)$ (it may be useful to see the JT in Figure~\ref{fig:jt_diagram}). This ensures that $\psi_T(S_T, \vec{A}_T)$ represents the true posterior incorporating all evidence in the model. Marginalising over the final latent RV $S_T$ yields:
\[
    p(\vec{A}_{1:T} \mid \Theta) = \sum_{S_T} \psi_T(S_T, \vec{A}_T),
\]
from which $\ell(\Theta) = \log p(\vec{A}_{1:T} \mid \Theta)$ follows directly.

\subsection{Model Output}
\label{sec:model_output}

The final point of discussion for this section revolves around the model outputs which were exported in two forms: \begin{itemize}
    \item Posterior decoding using the maximum posterior marginal (MPM) rule.
    \item State sequence decoding using the Viterbi algorithm.
\end{itemize}

\paragraph{1. MPM rule}
The MPM rule involves computing the point-wise marginal for the latent drought states $S_t$ at each time step:
\[
    \hat{s}_t = \underset{s}{\operatorname{argmax}} \;\; p(S_t = s \mid \vec{A}_{1:T}, \Theta),
\]
which is obtained using the calibrated JT outlined in Section~\ref{sec:inference}~\cite{murphy_2012,koller_textbook}. 

Conceptually, this rule will pick the most likely state, with a confidence attached, at each time step independently. This is particularly useful in quantifying the model's uncertainty when making particular classifications. It should be noted that this rule often leads to an unlikely or even impossible state sequence. For example, say $S_t = m$ (which signals very wet conditions) is then followed by $S_{t+1} = 1$ (which signals very dry conditions) in the very next time step.

\paragraph{2. Viterbi Algorithm}
On the other hand, the Viterbi algorithm produces a temporally coherent sequence that respects the state transition dynamics. Mathematically, the Viterbi decoding produces the single most probable joint state sequence:
\[
    \vec{s}^* = \underset{\vec{S}_{1:T}}{\operatorname{argmax}} \;\; p(\vec{S}_{1:T}\mid \vec{A}_{1:T}, \Theta).
\]
This decoding provides a more realistic representation of drought progression~\cite{viterbi}.  


\section{Implementation}
\label{sec:model_implementation}

Having established the structure, inference mechanisms, and parameter estimation procedures of the proposed model, this section describes its practical implementation which translates the formulations above into code. The discussion that follows focuses on the key design decisions and computational procedures rather than the underlying mathematics, which have already been presented in Section~\ref{sec:model_development}.

\subsection{Programming Environment and Tools}
All aspects of the DNBC model were implemented in \texttt{C++}, primarily chosen for its computational efficiency and the availability of the \texttt{emdw} library. This library, developed by Johan du Preez and Corné van Daalen, is still under development but provides robust functionality for probabilistic graphical models. The \texttt{C++} implementation handled the construction of factors, JT message passing, parameter estimation, model selection, and extraction of posterior outputs in both forms described in Section~\ref{sec:model_output}.

Python was used to complement this workflow, particularly for data-related tasks including raw data extraction, preprocessing into model inputs, postprocessing of model outputs, and visualisation of results. The primary Python libraries used were \texttt{numpy} for numerical computations, \texttt{pandas} for structured data handling, transformation, and aggregation, and \texttt{matplotlib} for generating all figures and visualisations.

This division allowed \texttt{C++} to focus on core model computation while Python streamlined data management and analysis for the rest of the project. 

\subsection{Model Implementation}
\label{sec:model_implementation2}
The model implementation consisted of two main components: the EM algorithm for parameter estimation, and model selection procedures for determining the optimal number of hidden drought states ($m$) and rolling-window size combination for the input indices SPI and SDI.

\subsubsection*{EM Algorithm}

The EM algorithm was implemented as specified in Section~\ref{sec:param_estimation}, with the following decisions made:
\begin{itemize}
    \item \textbf{Initialisation:} Model parameters were randomly initialised from a standard Gaussian distribution following common practice~\cite{moon_tk}.
    \item \textbf{Convergence criteria:} The algorithm was terminated when the relative change in log-likelihood satisfied:
    \[
        \frac{|\ell(\Theta)^{\text{new}} - \ell(\Theta)^{\text{old}}|}{|\ell(\Theta)^{\text{old}}|} < 10^{-4},
    \]
    or when a maximum of 100 iterations was reached.
\end{itemize}

The corresponding pseudocode (Algorithm~\ref{alg:em}) summarises this process and is included for completeness.

\subsubsection*{Model Selection}
\label{sec:model_selection_implementation}

Two independent model selection procedures were conducted. The first procedure involved sweeping the number of hidden states, $m$, from 2 to 10. For each competing model, the AIC, BIC, and maximum log-likelihood were computed. The second procedure evaluated combinations of rolling-window sizes for the SPI and SDI indices, each allowed to take values of 3-, 6-, 9-, and 12-month windows. Competing models were compared solely using the maximum log-likelihood criterion.

As discussed in Section~\ref{sec:model_selection}, these two groups of hyperparameters are decoupled meaning that varying one has no effect on the other. Accordingly, the first procedure used a standard 3-month rolling window for both SPI and SDI indices, while the second procedure was done with the optimal value for $m$ found in the first. The general framework below was applied to both procedures:
\begin{enumerate}
    \item Each candidate configuration was run with $10$ random restarts to mitigate local optima.
    \item The run yielding the highest log-likelihood was retained and used as the maximised log-likelihood, $L(\Theta)$.
    \item This maximised value was then used to compute AIC and BIC (where applicable) and to rank competing configurations.
\end{enumerate}

